---
title: "Faculty Classification"
author: Chad Evans
output: 
  github_document:
  toc: true
always_allow_html: yes
params:
 d: !r Sys.Date() 

---
Built with R version `r getRversion()`.  Last run on `r params$d`.

## Contents
* [Configure](#configure)
    + Directories
    + Libraries
* [Munge](#munge)
    + Subset
    + Missing Data
    + Variables
    + Imputation
    + Data
* [Cluster Analysis](#cluster-analysis)
    + Determining the Number of Clusters
    + K-means Clustering
* Tabulations
    + [Demography Table](#demography-table)
    + [Institution Table](#institution-table)
    + [Department Table](#departmental-table)
    + [Employment Table](#employment-table)

## Configure
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, fig.width=7, fig.height=7, fig.path='graphs/', cache.path ='cache/')
```

```{r Directories, include=FALSE}
Private_Cache <- "/Users/chadgevans/Research/Projects/Data/HERI/Cache"
Raw<-"/Users/chadgevans/Research/Projects/Data/HERI/Raw"
Munge<-"./munge"
Source<-"./src"
Graph<-"./graphs"
Libraries<-"./lib"
```

```{r Libraries, include=FALSE, eval=TRUE, echo=TRUE, warning=TRUE,error=FALSE, message=TRUE, tidy=TRUE, results='markup', cache=FALSE, fig.width=7, fig.height=7}
library(tidyverse)
library(stats)
library(mice)
library(knitr)
library(stats)
source(file.path(Libraries, "nfCrossTable.R"))
```

## Munge
```{r Munge}
load(file.path(Private_Cache,"HERI_Class.RData"))
source(file.path(Munge, "01_Merge_the_data.R"))
source(file.path(Munge, "02_Clean_the_data.R"))
source(file.path(Munge, "HERI_vars.R"))
```

```{r Subset_the_data, eval=FALSE}
raw_n<-nrow(df)
df<-df %>% filter(!(TENURE=="Tenured")) # reduces from 8980 to 8450
filtered_n<-nrow(df)
```

The original 2010 HERI data had 8980 observations.  After removing part-time faculty with Tenure (an anomoly outside the scope of this research), we have a total of 8450 faculty.

### Missing Data

```{r eval=T}
miss_pct<-df %>%
  map_dbl(function(x) { round((sum(is.na(x)) / length(x)) * 100, 1) })
data.frame(miss=miss_pct, var=names(miss_pct), row.names=NULL) %>%
ggplot(aes(x=reorder(var, -miss), y=miss)) +
geom_bar(stat='identity', fill='red') +
labs(x='', y='% missing', title='Percent missing data by feature') +
theme(axis.text.x=element_text(angle=90, hjust=1))
```

The missingness in these data is less concerning than it appears.  There are 31 variables missing more than 50 percent of their observations; however, this is because they are mostly only questions that pertain to part-time faculty.  Full-time faculty did not respond to this battery of questions and so these questions will not play any role in the analysis.  There are an additional 10 variables with 5-10 percent missingness.  This is not ideal, but the level of missingness is pretty small.  The vast majority of our variables (112 of them) have less than 5 percent missingness.  These will be very useful in helping identify clusters of faculty with these characteristics.  Despite this small amount of missingness, I still anticipate imputing data.  With so many features, surely listwise deletion would lead to an unacceptable deletion of most of our data.

```{r Variables}
df<-df %>% select(one_of(c(ADMINVARS,WORKVARS,SALARYVARS,INSTVARS,BACKVARS,ATTITUDEVARS,OTHERVARS,PROFDEVVARS, FACTORVARS, PRODUCTIVITYVARS,STRESSVARS,SATISVARS,PTVARS)))
HIGHMISS<-labels(which(miss_pct>50))
OMIT<-c(ADMINVARS,PTVARS,HIGHMISS) 

df$SALARYALL=pmax(df$SALARY, df$PTSALARY, na.rm = TRUE) # important to include the combined salary variable so that salary insn't exclude from the clustering.

df<-df %>% select(-one_of(OMIT))
```

We will conduct k-means clustering using as many meaningful faculty features as possible.  We remove the administrative variables like subject IDs and institution IDs that contribute no meaningful information.  We also remove all variables corresponding only to part-time faculty and the handful of other characterstics with missingness greater than 50 percent.  Our final data frame for k-means clustering consists of 8450 observations and 113 faculty features.

```{r, eval=FALSE}
#Imputing takes 2-3 hours
tempData<-df %>% select(-one_of(OMIT)) %>% mice(m=1,maxit=50,seed=500) # originally used meth='pmm'
#save(tempData, file=file.path(Private_Cache,"tempData.RData"))
IData <- complete(tempData,1)
save(IData, file=file.path(Private_Cache,"IData.RData"))
```

As expected, listwise deletion across 117 features is impossible.  In fact, there is not a single observation with complete data.  We thus must consider a method to deal with the missingness.  I opt for single imputiation.  In some cases, like regression, it would probably be worth multiply imputing to get standard errors correct.  However, this study is not using standard errors and is merely an experimental procedure to find coherence in the data. I thus opt to singly impute the data for reasons of simplicity.

I set the maximum iterations to 50, rather than the default of 5.  This will give the chained equations more attempts to converge on a good imputed value for each cell.  Predictive mean matching was used for numeric variables. Logistic regression was used to impute binary data.  Polytomous regression imputation was used for unordered categorical variables.

## Data
```{r reload_data}
load(file.path(Private_Cache,"IDataOLD.RData")) # Singly imputed data
data<-IData
```

##Cluster Analysis

```{r Create_Binaries}
data<-data.frame(model.matrix(~ ., data=data , contrasts.arg = lapply(data[,sapply(data, is.factor)], contrasts, contrasts=FALSE)))
```

To implement k-means clustering, all data must be numeric.  This required converting binary factors to zero and ones.  Multinomial variables needed to be converted into a matrix of dummy variables.  Some claim that it is ineffective to convert categorical predictors into binaries in this fashion.  But it is necessary if you want multinomial features to factor into the analysis.  As this chapter is descriptive and exploratory, I implemented this traditional practice of creating a matrix of numeric variables.

### Determining the number of Clusters.

When implementing k-means clustering, one must specify the number of means to cluster around in the data.  The most common approach to choosing the number of clusters is to plot how the within sum of squared residuals decreases as additional means are added and identify the "elbow."  This is the point where explained variation begins its platau.

```{r Determining_Number_Clusters}
wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
  }
  plot(1:nc, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares", main="Determining the Number of Clusters")
}
wssplot(data, nc=7) # put in # of clusters here
```

The elbow suggests that four clusters sufficiently explain most of the variation.  I therefore opt to go witih four means in the k-means clustering analysis.

### K-Means Clustering
```{r K_means_procedure}
d<-data[,-1] # git rid of the intercept (no variation, convergence issues)
d<-scale(d)
kmeans.obj<- d %>% kmeans(4, nstart = 10, algorithm = c("Hartigan-Wong"))
df$cluster<-kmeans.obj$cluster
```

Before conducting k-means clustering, all variables were normalized so that features with the greatest ranges did not have undue influence on the formation of clusters.

To conduct the k-means analysis, it is important to choose random starting points for the means.  This helps prevent the algorithm (Hartingan-Wong 1979) from converging on suboptimal means.  I used 10 different sets of starting points to identify the means that best summarize the information in the data.

## Tabulations
```{r Table_Recode}
source(file.path(Munge, "03_Recode_HERI.R"))
```

```{r}
clusters<-table(df$cluster)
print(clusters)
C1<-paste("Cluster 1 (n=",clusters[1],")",sep = "")
C2<-paste("Cluster 2 (n=",clusters[2],")",sep = "")
C3<-paste("Cluster 3 (n=",clusters[3],")",sep = "")
C4<-paste("Cluster 4 (n=",clusters[4],")",sep = "")
Clusternames<-c(C1,C2,C3,C4)
```

### Demography Table

```{r demography_table}
DemVars<-c("AGE","SEX","MARITAL2","NCHILD3","RACEGROUP2","GENACT02","NATENGSP","DEGEARN2","DEGWORK2")
table<-round(nfCrossTable(data=df[DemVars],CTvar=df$cluster),2)
colnames(table)<-Clusternames
rownames(table)<-c("Age","Male","Married","No Children","One Child","Multiple Children","White","Citizen","Native English","BA or Less","Prof Degree","Ph.D.","Working on a Degree")
kable(table, caption = "Distribution of Adjunct Clusters by Demographic Characteristics")
```

### Institution Table

```{r institution_table}
INSTVARS<-c("INSTTYPE","INSTCONT","CARNEGIE","BIGLAN","SELECTIVITY2","INSTDESCR03","INSTDESCR08","INSTOPN10","INSTOPN11")
table<-round(nfCrossTable(data=df[INSTVARS],CTvar=df$cluster),2)
colnames(table)<-Clusternames
rownames(table)<-c("2-year","4-year","University","Public","Research I","Research II","R3/Doctoral","Bachelors/Masters","Associates","Other Inst.","Hard/Applied","Hard/Pure","Soft/Applied","Soft/Pure","Other Biglan","Highly Selective","Faculty very respectful","Administators very considerate","Research valued","Teaching valued")
kable(table, caption = "Distribution of Adjunct Clusters by Institutional Characteristics")
```

### Departmental Table

```{r dept_table}
table<-round(prop.table(table(df$DEPTA, df$cluster),2),2) # DEPTA was aggregated by HERI
colnames(table)<-Clusternames
rownames(table)<-c("Agri/Forestry","Biology","Business","Education","Engineering","English","Fine Arts","Health-related","History/PoliSci","Humanities","Math/Stats","Non-technical","Technical","Physical Sciences","Social Sciences")
kable(table, caption = "Distribution of Adjunct Clusters by Departmental Characteristics")
```

### Employment Table

```{r Employment_Conditions}
WORKVARS<-c("PRINACT2","FULLSTAT","ACADRANK","GENACT01","HEALTHBENEFITS", "RETIREBENEFITS","SALARYALL","COURSENUM","PROFDEVFAC")
table<-round(nfCrossTable(data=df[WORKVARS],CTvar=df$cluster),2)
colnames(table)<-Clusternames
rownames(table)<-c("Teaching","Research","Administration","Other","Full-time","Assistant Professor","Associate Professor","Instructor","Lecturer","Professor","Union member","Health benefits","Retirement","Avg. Salary","Avg. Courses","Prof. Dev. Rating")
kable(table, caption = "Distribution of Adjunct Clusters by Work Characteristics")

```
